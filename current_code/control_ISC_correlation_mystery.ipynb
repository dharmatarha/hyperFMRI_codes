{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nThis script loads the timeseries data from the listening and reading\\nhyperscanning control tasks, and visualizes the data in various ways\\nwith the goal of trying to elucidate why we see so many negative\\ncorrelations between individual timeseries and averaged timeseries\\nfrom the rest of the group.\\n\\nTo add:\\nlook at just primary auditory cortex\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "This script loads the timeseries data from the listening and reading\n",
    "hyperscanning control tasks, and visualizes the data in various ways\n",
    "with the goal of trying to elucidate why we see so many negative\n",
    "correlations between individual timeseries and averaged timeseries\n",
    "from the rest of the group.\n",
    "\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "server = 1 # 0=drzeuss, 1=discovery\n",
    "\n",
    "if server == 0: # drzeuss\n",
    "    baseFolder = '/afs/dbic.dartmouth.edu/usr/wheatley/jd'\n",
    "    sys.path.append(baseFolder)\n",
    "    inputFolder = baseFolder + 'control_tasks/'\n",
    "    maskFile = '/flash/wheatley/adamb/mni_asym09c_mask_resamp3x3.nii.gz'\n",
    "    statMapFolder = inputFolder\n",
    "    htmlFolder = inputFolder\n",
    "else: # discovery\n",
    "    baseFolder = '/dartfs-hpc/rc/home/z/f00589z/hyperscanning/'\n",
    "    sys.path.append(baseFolder + 'support_scripts/')\n",
    "    inputFolder = baseFolder + 'control_tasks/nuisRegr_output_files/'\n",
    "    maskFile = baseFolder + 'control_tasks/nuisRegr_input_files/mni_asym09c_mask_resamp3x3.nii.gz'\n",
    "    statMapFolder = baseFolder + 'control_tasks/statMaps/'\n",
    "    htmlFolder = baseFolder + 'control_tasks'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% set server on which to run the script and set the associated paths\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.multitest as multi\n",
    "from phaseScramble import *\n",
    "from CircleShift import *\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pairNum      subID\n",
      "0         2  sid000007\n",
      "1         3  sid000009\n",
      "2         4  sid000560\n",
      "3         5  sid000535\n",
      "4         6  sid000102\n",
      "5         7  sid000416\n",
      "6         8  sid000499\n",
      "7         9  sid000142\n",
      "8         2  hid000002\n",
      "9         3  hid000003\n",
      "10        4  hid000004\n",
      "11        5  hid000005\n",
      "12        6  hid000006\n",
      "13        7  hid000007\n",
      "14        8  hid000008\n",
      "15        9  hid000009\n"
     ]
    }
   ],
   "source": [
    "# participant IDs\n",
    "dbicIDs = np.array([\"sid000007\", \"sid000009\", \"sid000560\", \"sid000535\", \"sid000102\", \"sid000416\", \"sid000499\", \"sid000142\"])\n",
    "cbsIDs = np.array([\"hid000002\", \"hid000003\", \"hid000004\", \"hid000005\", \"hid000006\", \"hid000007\", \"hid000008\", \"hid000009\"])\n",
    "\n",
    "# pair numbers\n",
    "pairNums = np.arange(2,len(dbicIDs)+2)\n",
    "\n",
    "# make subject list data frame\n",
    "subList = pd.DataFrame(np.transpose(np.tile(pairNums, (1, 2))),columns=['pairNum'])\n",
    "subList['subID'] = np.concatenate((dbicIDs, cbsIDs), axis=0)\n",
    "print(subList)\n",
    "\n",
    "# get number of participants\n",
    "numSubs = len(pairNums) * 2\n",
    "\n",
    "# set fitting distribution to normal\n",
    "dist = getattr(stats, 'norm')\n",
    "\n",
    "# define condition labels\n",
    "taskNames = np.array(['listening','reading'])\n",
    "normNames = np.array(['',', norm'])\n",
    "siteNames = np.array(['DBIC','CBS'])\n",
    "\n",
    "# set scaling type\n",
    "standardScaling = True # True = mean center and z-score, False = normalize to range from 0 to 1\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# set whether or not to export html\n",
    "exportHtml = False\n",
    "\n",
    "# remove some set of initial time points from each task?\n",
    "# Note: this was just a quick hacky way to check how removing the time points\n",
    "# over which that big initial drift occurs will affect ISC\n",
    "RI = False\n",
    "removeInitial = [100,50] # number of timepoints to remove off the front\n",
    "\n",
    "# save out drfit and smoothness stat maps\n",
    "saveStatMaps = False\n",
    "\n",
    "# # sign flip harvard?\n",
    "# SFH = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% setup\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 508 x 69880 timeseries for listening task, sub sid000007\n",
      "loaded 506 x 69880 timeseries for listening task, sub sid000009\n",
      "loaded 533 x 69880 timeseries for listening task, sub sid000560\n",
      "loaded 501 x 69880 timeseries for listening task, sub sid000535\n",
      "loaded 561 x 69880 timeseries for listening task, sub sid000102\n",
      "loaded 500 x 69880 timeseries for listening task, sub sid000416\n",
      "loaded 504 x 69880 timeseries for listening task, sub sid000499\n",
      "loaded 503 x 69880 timeseries for listening task, sub sid000142\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000002\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000003\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000004\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000005\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000006\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000007\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000008\n",
      "loaded 510 x 69880 timeseries for listening task, sub hid000009\n",
      "loaded 482 x 69880 timeseries for reading task, sub sid000007\n",
      "loaded 481 x 69880 timeseries for reading task, sub sid000009\n",
      "loaded 480 x 69880 timeseries for reading task, sub sid000560\n",
      "loaded 481 x 69880 timeseries for reading task, sub sid000535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-5e1d38677085>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mNORM\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m                 \u001B[0;31m# load real data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m                 \u001B[0mtmp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadmat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfileName\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#load file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m                 \u001B[0mboldData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTASK\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mNORM\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mSUB\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtmp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'tseries'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m#get timeseries data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'loaded '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mboldData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTASK\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mNORM\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mSUB\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m' x '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mboldData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTASK\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mNORM\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mSUB\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m' timeseries for '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mtaskNames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTASK\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m' task, sub '\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0msubList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'subID'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mSUB\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hypeScanCentral/lib/python3.9/site-packages/scipy/io/matlab/mio.py\u001B[0m in \u001B[0;36mloadmat\u001B[0;34m(file_name, mdict, appendmat, **kwargs)\u001B[0m\n\u001B[1;32m    224\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0m_open_file_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mappendmat\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m         \u001B[0mMR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmat_reader_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 226\u001B[0;31m         \u001B[0mmatfile_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMR\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvariable_names\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mmdict\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hypeScanCentral/lib/python3.9/site-packages/scipy/io/matlab/mio5.py\u001B[0m in \u001B[0;36mget_variables\u001B[0;34m(self, variable_names)\u001B[0m\n\u001B[1;32m    331\u001B[0m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    332\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 333\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_var_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhdr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprocess\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    334\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mMatReadError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    335\u001B[0m                 warnings.warn(\n",
      "\u001B[0;32m~/.conda/envs/hypeScanCentral/lib/python3.9/site-packages/scipy/io/matlab/mio5.py\u001B[0m in \u001B[0;36mread_var_array\u001B[0;34m(self, header, process)\u001B[0m\n\u001B[1;32m    291\u001B[0m            \u001B[0;31m`\u001B[0m\u001B[0mprocess\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    292\u001B[0m         '''\n\u001B[0;32m--> 293\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_matrix_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray_from_header\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprocess\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    294\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    295\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvariable_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# set input data folder\n",
    "\n",
    "\n",
    "# add a filename tag (optional)\n",
    "inputFileTag = True\n",
    "if inputFileTag:\n",
    "    fileTag = '_detrended'\n",
    "else:\n",
    "    fileTag = ''\n",
    "\n",
    "# preallocate task lists\n",
    "boldData = [[]] * 2\n",
    "\n",
    "# get EPI time series\n",
    "for TASK in [0,1]: # for each task, listening, then reading...\n",
    "\n",
    "    # preallocate normalization lists\n",
    "    boldData[TASK] = [[]] * 2\n",
    "\n",
    "    for NORM in [0,1]: # for each normalization condition (0 = no normalization, 1 = normalization)...\n",
    "\n",
    "        # preallocate subject lists\n",
    "        boldData[TASK][NORM] = [[]] * numSubs\n",
    "\n",
    "        for SUB in range(numSubs): # for each subject...\n",
    "\n",
    "            # get file name\n",
    "            fileName = inputFolder + 'sub-' + subList['subID'][SUB] + '_ses-pair0' + str(subList['pairNum'][SUB]) + '_task-storytelling' + str(TASK + 3) + '_run-0' + str(TASK + 3) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021' + fileTag + '.mat'\n",
    "\n",
    "            if NORM == 0:\n",
    "                # load real data\n",
    "                tmp = sio.loadmat(fileName) #load file\n",
    "                boldData[TASK][NORM][SUB] = tmp['tseries'] #get timeseries data\n",
    "                print('loaded ' + str(boldData[TASK][NORM][SUB].shape[0]) + ' x ' + str(boldData[TASK][NORM][SUB].shape[1]) + ' timeseries for ' + taskNames[TASK] + ' task, sub ' + subList['subID'][SUB])\n",
    "\n",
    "            else:\n",
    "\n",
    "                # normalize\n",
    "                if standardScaling:\n",
    "                    boldData[TASK][NORM][SUB] = scaler.fit_transform(boldData[TASK][0][SUB])\n",
    "                else:\n",
    "                    boldData[TASK][NORM][SUB] = preprocessing.normalize(boldData[TASK][0][SUB])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load listening and reading timeseries\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if RI:\n",
    "    for TASK in [0,1]: # for each task, listening, then reading\n",
    "        for NORM in [0,1]: # for each normalization condition (0 = no normalization, 1 = normalization)\n",
    "            for SUB in range(numSubs): # for each subject\n",
    "                boldData[TASK][NORM][SUB] = np.delete(boldData[TASK][NORM][SUB], np.arange(removeInitial[TASK]), 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% truncate time series if applicable\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrData = [[]] * 2\n",
    "for TASK in [0,1]: # for each task, listening, then reading\n",
    "\n",
    "    corrData[TASK] = [[]] * 2\n",
    "\n",
    "    for NORM in [0,1]: # for each normalization condition (not normalized, normalized)\n",
    "\n",
    "        # preallocate task data list\n",
    "        corrData[TASK][NORM] = [[]] * numSubs\n",
    "\n",
    "        for SUB in range(numSubs): #for each subject\n",
    "\n",
    "            # get mean of data from all participants EXCEPT the current participant\n",
    "            otherSubs = np.arange(0,numSubs)\n",
    "            otherSubs = np.delete(otherSubs,SUB)\n",
    "            groupMean = np.mean([boldData[TASK][NORM][i] for i in otherSubs], axis=0)\n",
    "\n",
    "            # get correlation between current participant and groupMean\n",
    "            corrData[TASK][NORM][SUB] = fastColumnCorr(boldData[TASK][NORM][SUB], groupMean)\n",
    "            print('computing correlation for sub ' + str(SUB + 1) + ', ' + taskNames[TASK] + ' task' + normNames[NORM])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% get correlations\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get number of pairs\n",
    "numPairs = round(numSubs / 2)\n",
    "\n",
    "# include random data at the bottom?\n",
    "randData = False\n",
    "\n",
    "# make subplot map\n",
    "if randData:\n",
    "    spMap = np.arange(10).reshape(5,2) + 1\n",
    "else:\n",
    "    spMap = np.arange(8).reshape(4,2) + 1\n",
    "\n",
    "# set axis label font size\n",
    "axLabFontSize = 12\n",
    "\n",
    "# colorblind-friendly colors list\n",
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "# set task colors\n",
    "taskColors = CB_color_cycle[:2]\n",
    "\n",
    "# plot data\n",
    "for SUB in range(numPairs):\n",
    "\n",
    "    # get subjects from current pair\n",
    "    pairSubs = [SUB,SUB + round((numSubs / 2))]\n",
    "\n",
    "    # initialize plot\n",
    "    plt.figure(facecolor='white',figsize=(6,14))\n",
    "\n",
    "    # for each subject in the current pair\n",
    "    for PAIRSUB in [0,1]:\n",
    "\n",
    "        for TASK in [0,1]:\n",
    "\n",
    "            for NORM in [0,1]:\n",
    "\n",
    "                # get plot data\n",
    "                pData = corrData[TASK][NORM][pairSubs[PAIRSUB]]\n",
    "\n",
    "                # select subplot\n",
    "                plt.subplot(5, 2, spMap[NORM+PAIRSUB*2,TASK])\n",
    "\n",
    "                # plot histogram\n",
    "                plt.hist(pData, bins=25, density=True, alpha=0.6, color=taskColors[TASK])\n",
    "\n",
    "                # dashed line at x=0\n",
    "                yMax = plt.gca().get_ylim()[1]\n",
    "                plt.plot([0, 0], [0, yMax], '--k')\n",
    "\n",
    "                # axes and title\n",
    "                plt.xlabel('correlation', fontsize=axLabFontSize)\n",
    "                if TASK == 0:\n",
    "                    plt.ylabel('voxel count', fontsize=axLabFontSize)\n",
    "                plt.title(taskNames[TASK] + ', sub ' + siteNames[PAIRSUB] + str(SUB + 1) + normNames[NORM])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% plot some histograms\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "corrMat = [[]] * 2\n",
    "corrColors = [[]] * 2\n",
    "corrData_pairs = [[]] * 2\n",
    "axLab = [[]] * numSubs\n",
    "\n",
    "for TASK in [0,1]:\n",
    "\n",
    "    corrMat[TASK] = [[]] * 2\n",
    "    corrColors[TASK]= [[]] * 2\n",
    "    corrData_pairs[TASK]= [[]] * 2\n",
    "\n",
    "    for NORM in [0,1]:\n",
    "\n",
    "        # some feedback\n",
    "        print('computing pairwise correlations for ' + str(taskNames[TASK]) + ' task' + normNames[NORM])\n",
    "\n",
    "        # preallocate subs x subs correlation matrix\n",
    "        corrMat[TASK][NORM] = np.empty([numSubs,numSubs])\n",
    "        corrData_pairs[TASK][NORM] = [[]] * numSubs\n",
    "\n",
    "        for SUB1 in range(numSubs):\n",
    "\n",
    "            corrData_pairs[TASK][NORM][SUB1] = [[]] * numSubs\n",
    "\n",
    "            # get axis labels\n",
    "            if TASK == 0 & NORM == 0:\n",
    "                if SUB1 < numPairs:\n",
    "                    axLab[SUB1] = 'D' + str(SUB1 + 1)\n",
    "                else:\n",
    "                    axLab[SUB1] = 'H' + str(SUB1 - numPairs + 1)\n",
    "\n",
    "            # set the diagonal equal to 1\n",
    "            corrMat[TASK][NORM][SUB1,SUB1] = 1\n",
    "\n",
    "            for SUB2 in np.arange(SUB1 + 1,numSubs):\n",
    "\n",
    "                corrData_pairs[TASK][NORM][SUB1][SUB2] = fastColumnCorr(boldData[TASK][NORM][SUB1], boldData[TASK][NORM][SUB2])\n",
    "                corrMat[TASK][NORM][SUB1,SUB2] = np.mean(corrData_pairs[TASK][NORM][SUB1][SUB2])\n",
    "\n",
    "                #fill in the other half of corrMat so the plots dont look weird\n",
    "                corrMat[TASK][NORM][SUB2,SUB1] = corrMat[TASK][NORM][SUB1,SUB2]\n",
    "\n",
    "\n",
    "        plt.figure(facecolor='white')\n",
    "        cmap = cm.get_cmap('RdBu')#sns.diverging_palette(20, 220, n=200)\n",
    "        ax = sns.heatmap(\n",
    "            corrMat[TASK][NORM],\n",
    "            vmin=-1, vmax=1, center=0,\n",
    "            cmap=cmap,\n",
    "            square=True\n",
    "        )\n",
    "        ax.set_xticklabels(axLab)\n",
    "        ax.set_xticklabels(\n",
    "            ax.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            horizontalalignment='right'\n",
    "        )\n",
    "        ax.set_yticklabels(axLab)\n",
    "        ax.set_yticklabels(\n",
    "            ax.get_yticklabels(),\n",
    "            rotation=0\n",
    "        )\n",
    "\n",
    "        # add a title\n",
    "        plt.title('mean corr coef across vox, ' + taskNames[TASK] + ' task' + normNames[NORM])\n",
    "\n",
    "        # get heatmap rgbs\n",
    "        im = ax.collections[0]\n",
    "        corrColors[TASK][NORM] = im.cmap(im.norm(im.get_array()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% sub x sub correlation matrices\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use normalized data to put things on the same scale\n",
    "NORM = 1\n",
    "\n",
    "# extreme voxel labels\n",
    "voxLabs = ['min corr vox','max corr vox','median vox']\n",
    "voxColors = ['y','m','k']\n",
    "\n",
    "# make subplotting map\n",
    "spMap3 = np.arange(8).reshape(4,2) + 1\n",
    "\n",
    "# plot data\n",
    "for SUB in range(numSubs):\n",
    "\n",
    "    # initialize plot\n",
    "    plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "    if SUB < numPairs:\n",
    "        titleString = dbicIDs[SUB] + normNames[NORM]\n",
    "    else:\n",
    "        titleString = cbsIDs[SUB - numPairs] + normNames[NORM]\n",
    "\n",
    "    plt.suptitle(titleString)\n",
    "\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        # get plot data\n",
    "        pData = corrData[TASK][NORM][SUB]\n",
    "\n",
    "        # select subplot for histogram\n",
    "        plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap[0,TASK])\n",
    "\n",
    "        # plot histogram\n",
    "        plt.hist(pData, bins=100, density=True, alpha=0.6, color=taskColors[TASK])\n",
    "\n",
    "        # dashed line at x=0\n",
    "        yMax = plt.gca().get_ylim()[1]\n",
    "        plt.plot([0, 0], [0, yMax], '--k')\n",
    "\n",
    "        # axes and title\n",
    "        plt.xlabel('correlation', fontsize=axLabFontSize)\n",
    "        if TASK == 0:\n",
    "            plt.ylabel('voxel count', fontsize=axLabFontSize)\n",
    "        plt.title(taskNames[TASK])\n",
    "\n",
    "        # plot voxel time series with extreme values\n",
    "        for VOX in [0,1,2]: # min, max, median\n",
    "\n",
    "            # get \"Extreme Index\" of voxel with either min or max value (or median)\n",
    "            if VOX == 0:\n",
    "                EIND = np.unravel_index(np.argmin(pData),pData.shape) # minimum correlation voxel index\n",
    "            elif VOX == 1:\n",
    "                EIND = np.unravel_index(np.argmax(pData),pData.shape) # maximum correlation voxel index\n",
    "            elif VOX == 2:\n",
    "                EIND = np.argsort(pData)[len(pData)//2] # median (approximately)\n",
    "\n",
    "            # add locations of min and max correlation to histogram for reference\n",
    "            extremeCorr = pData[EIND]\n",
    "            plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[0,TASK])\n",
    "            plt.plot([extremeCorr, extremeCorr], [0, yMax], '-' + voxColors[VOX])\n",
    "\n",
    "            # get individual subject time series at the extreme voxel\n",
    "            y1 = boldData[TASK][NORM][SUB][:,EIND]\n",
    "            x = np.array(range(len(y1))) + 1\n",
    "\n",
    "            # get mean of data from all participants EXCEPT the current participant\n",
    "            otherSubs = np.arange(0,numSubs)\n",
    "            otherSubs = np.delete(otherSubs,SUB)\n",
    "            y2 = np.mean([boldData[TASK][NORM][i][:,EIND] for i in otherSubs], axis=0)\n",
    "            if VOX == 2: #hack to deal with EIND not being a tuple when we find the median\n",
    "                y2 = y2.reshape(y2.shape[0],1)\n",
    "            y2 = scaler.fit_transform(y2) # normalize the rest-of-group mean (see next section for confirmation that this doesn't influence correlations)\n",
    "\n",
    "            # select subplot and reset subplot border color\n",
    "            ax = plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[VOX + 1,TASK])\n",
    "            plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "            plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "            # plot lines and add legend\n",
    "            line1, = plt.plot(x,y1,'-k',label = 'individual')\n",
    "            line2, = plt.plot(x,y2,'-', label = 'rest of group', color = taskColors[TASK]) # , linewidth=2\n",
    "            plt.legend(handles=[line1, line2],loc='upper right')\n",
    "\n",
    "            if TASK == 0:\n",
    "                plt.xlabel('TR')\n",
    "            else:\n",
    "                plt.xlabel('reading stimulus flip')\n",
    "            plt.ylabel('BOLD signal')\n",
    "            plt.title(voxLabs[VOX])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% look at sub vs group timeseries in individual voxels\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_nearest_percentile_index(array, percentile):\n",
    "    array = np.asarray(array)\n",
    "    target = np.percentile(array, percentile)\n",
    "    idx = (np.abs(array - target)).argmin()\n",
    "    return idx\n",
    "\n",
    "voxMethod = 'median'\n",
    "\n",
    "spMap = np.arange(6).reshape(3,2) + 1\n",
    "\n",
    "NORM = 1\n",
    "numVox = boldData[0][0][0].shape[1] # get number of voxels from first sub\n",
    "smoothness = [[]] * 2 # preallocate\n",
    "stdSmooth = False\n",
    "voxColors = CB_color_cycle[4:9]\n",
    "percentiles = [1, 2, 3]\n",
    "smoothLabs = ['max smoothness',str(percentiles[0]) + ' %', str(percentiles[1]) + ' %', str(percentiles[2]) + ' %','min smoothness']\n",
    "for TASK in [0,1]: # for each task\n",
    "    smoothness[TASK] = [[]] * numSubs # preallocate\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        # initialize plot\n",
    "        plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "        # main title\n",
    "        plt.suptitle(taskNames[TASK] + ' sub ' + str(SUB + 1))\n",
    "\n",
    "        # get data\n",
    "        data = boldData[TASK][NORM][SUB]\n",
    "        smoothness[TASK][SUB] = np.std(np.diff(data,axis=0),axis=0) / abs(np.mean(np.diff(data,axis=0),axis=0)) # formula should be intuitive, but got it from here: https://stats.stackexchange.com/questions/24607/how-to-measure-smoothness-of-a-time-series-in-r\n",
    "\n",
    "        # optional standardization\n",
    "        if stdSmooth:\n",
    "            smoothness[TASK][SUB] = (smoothness[TASK][SUB] - np.mean(smoothness[TASK][SUB])) / np.std(smoothness[TASK][SUB])\n",
    "\n",
    "        # arbitrarily subset for plotability (because these are so skewed)\n",
    "        plotDataInds = np.argwhere(smoothness[TASK][SUB] < 1000)\n",
    "        plotData = smoothness[TASK][SUB][plotDataInds]\n",
    "\n",
    "        # select subplot for histogram\n",
    "        plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "        plt.hold(True)\n",
    "\n",
    "        # plot smoothness histogram\n",
    "        plt.hist(plotData, bins=100, density=True, alpha=1, color=taskColors[TASK])\n",
    "        plt.xlabel('inverse smoothness parameter')\n",
    "        plt.ylabel('proportion of voxels')\n",
    "\n",
    "        # get voxel indices for time series with various levels of smoothness smoothness\n",
    "        evox = [[]] * 5\n",
    "        evox[0] = np.unravel_index(np.argmin(plotData),plotData.shape)[0]\n",
    "        if voxMethod == 'median':\n",
    "            evox[1] = (np.abs(plotData - (np.median(plotData) - np.std(plotData)))).argmin()\n",
    "            evox[2] = (np.abs(plotData - np.median(plotData))).argmin()\n",
    "            evox[3] = (np.abs(plotData - (np.median(plotData) + np.std(plotData)))).argmin()\n",
    "        else:\n",
    "            counter = 1\n",
    "            for PERC in percentiles:\n",
    "                evox[counter] = find_nearest_percentile_index(plotData, PERC)\n",
    "                counter += 1\n",
    "        evox[4] = np.unravel_index(np.argmax(plotData),plotData.shape)[0]\n",
    "\n",
    "        # get histogram max y-value\n",
    "        yMax = plt.gca().get_ylim()[1]\n",
    "\n",
    "        # plot single voxel timeseries\n",
    "        for VOX in range(len(evox)):\n",
    "\n",
    "            # add vertical bars to histogram\n",
    "            plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "            smoothVal = plotData[evox[VOX]]\n",
    "            plt.plot([smoothVal, smoothVal], [0, yMax], '-', color=voxColors[VOX])\n",
    "\n",
    "            # get time series at the extreme voxel\n",
    "            y = boldData[TASK][NORM][SUB][:,plotDataInds[evox[VOX]]]\n",
    "            x = np.array(range(len(y))) + 1\n",
    "\n",
    "            # select subplot for time series line plot\n",
    "            ax = plt.subplot(spMap.shape[0], spMap.shape[1], VOX + 2)\n",
    "            plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "            plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "            # plot time series\n",
    "            plt.plot(x,y,'-k')\n",
    "\n",
    "            # subplot title and axis labels\n",
    "            plt.title(smoothLabs[VOX])\n",
    "            if TASK == 0:\n",
    "                plt.xlabel('TR')\n",
    "            else:\n",
    "                plt.xlabel('reading stimulus flip')\n",
    "            plt.ylabel('BOLD signal')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% look at smoothness\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a numSubs by numSubs plot map\n",
    "spMap4 = np.arange(numSubs**2).reshape(numSubs,numSubs)\n",
    "\n",
    "# set plot width [inches?]\n",
    "plotWidth = 16\n",
    "\n",
    "# hardcode normalization because 64 plots is enough\n",
    "NORM = 1\n",
    "\n",
    "# plot data\n",
    "for SUB1 in range(numSubs):\n",
    "\n",
    "    # get sub1 string\n",
    "    if SUB1 < numPairs:\n",
    "        sub1Str = 'D' + str(SUB1 + 1)\n",
    "    else:\n",
    "        sub1Str = 'H' + str(SUB1 - numPairs + 1)\n",
    "\n",
    "    for SUB2 in np.arange(SUB1 + 1,numSubs):\n",
    "\n",
    "        # get sub2 string\n",
    "        if SUB2 < numPairs:\n",
    "            sub2Str = 'D' + str(SUB2 + 1)\n",
    "        else:\n",
    "            sub2Str = 'H' + str(SUB2 - numPairs + 1)\n",
    "\n",
    "        # initialize plot\n",
    "        plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "        # main title\n",
    "        plt.suptitle('subs ' + sub1Str + ' & ' + sub2Str)\n",
    "\n",
    "        for TASK in [0,1]:\n",
    "\n",
    "            # get data from voxels of interest\n",
    "            pData = corrData_pairs[TASK][NORM][SUB1][SUB2]\n",
    "\n",
    "            # plot histogram\n",
    "            plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap[0,TASK])\n",
    "            plt.hist(pData, bins=100, density=True, alpha=0.6, color=taskColors[TASK])\n",
    "\n",
    "            # dashed line at x=0\n",
    "            yMax = plt.gca().get_ylim()[1]\n",
    "            plt.plot([0, 0], [0, yMax], '--k')\n",
    "\n",
    "            # axes and title\n",
    "            plt.xlabel('correlation', fontsize=axLabFontSize)\n",
    "            if TASK == 0:\n",
    "                plt.ylabel('voxel count', fontsize=axLabFontSize)\n",
    "            plt.title(taskNames[TASK])\n",
    "\n",
    "            for VOX in [0,1,2]: # min, max, median\n",
    "\n",
    "                # get \"Extreme Index\" of voxel with either min or max value (or median)\n",
    "                if VOX == 0:\n",
    "                    EIND = np.unravel_index(np.argmin(pData),pData.shape) # minimum correlation voxel index\n",
    "                elif VOX == 1:\n",
    "                    EIND = np.unravel_index(np.argmax(pData),pData.shape) # maximum correlation voxel index\n",
    "                elif VOX == 2:\n",
    "                    EIND = np.argsort(pData)[len(pData)//2] # median (approximately)\n",
    "\n",
    "                # add locations of min and max correlation to histogram for reference\n",
    "                extremeCorr = pData[EIND]\n",
    "                plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[0,TASK])\n",
    "                plt.plot([extremeCorr, extremeCorr], [0, yMax], '-', color=voxColors[VOX])\n",
    "\n",
    "                # get individual subject time series at the extreme voxel\n",
    "                y1 = boldData[TASK][NORM][SUB1][:,EIND]\n",
    "                y2 = boldData[TASK][NORM][SUB2][:,EIND]\n",
    "                x = np.array(range(len(y1))) + 1\n",
    "\n",
    "                # select subplot for time series line plot\n",
    "                ax = plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[VOX + 1,TASK])\n",
    "                plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "                plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "                line1, = plt.plot(x,y1,'-k',label = sub1Str)\n",
    "                line2, = plt.plot(x,y2,'-', label = sub2Str, color = taskColors[TASK])\n",
    "                plt.legend(handles=[line1, line2],loc='upper right')\n",
    "\n",
    "                if TASK == 0:\n",
    "                    plt.xlabel('TR')\n",
    "                else:\n",
    "                    plt.xlabel('reading stimulus flip')\n",
    "                plt.ylabel('BOLD signal')\n",
    "                plt.title(voxLabs[VOX])\n",
    "\n",
    "        # display plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% pairwise version of individual voxel time series comparisons\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compare the mean signal of an early epoch to that of a late epoch. Greater absolute differences\n",
    "# should indicate greater drift. NOTE that this is super hacky, but should be FAST and at least\n",
    "# somewhat sensitive\n",
    "\n",
    "# set ending and starting time points for the early and late epochs for each task, respectively\n",
    "# epochBorders = [[10,5],[100,50]] would mean...\n",
    "# early epochs for the listening and reading tasks would be time points 1-10 and 1-5, respectively\n",
    "# and the late epochs would be 100-end, 50-end, also respectively\n",
    "epochBorders = [[10,5],[100,50]]\n",
    "\n",
    "# subplot map\n",
    "spMap = np.arange(6).reshape(3,2) + 1\n",
    "\n",
    "# use normalized data\n",
    "NORM = 1\n",
    "\n",
    "# set percentiles, colors, labels\n",
    "voxColors = CB_color_cycle[4:9]\n",
    "percentiles = [10, 50, 90]\n",
    "if voxMethod == 'mean':\n",
    "    diffLabs = ['most negative diff','mean - SD','mean','mean + SD','most negative diff']\n",
    "else:\n",
    "    diffLabs = ['most negative diff',str(percentiles[0]) + ' %', str(percentiles[1]) + ' %', str(percentiles[2]) + ' %','most positive diff']\n",
    "\n",
    "# standardize difference scores\n",
    "stdDiff = True\n",
    "\n",
    "# preallocate arrays\n",
    "epoch = [[]] * 2\n",
    "driftHack = [[]] * 2\n",
    "\n",
    "# get number of samples in the time series from each task, using the normalized data from the first subject\n",
    "numSamps = [boldData[0][0][0].shape[0], boldData[1][0][0].shape[0]]\n",
    "\n",
    "for TASK in [0,1]:\n",
    "\n",
    "    # get epoch time points\n",
    "    epoch[TASK] = [[]] * 2 # preallocate\n",
    "    epoch[TASK][0] = np.arange(epochBorders[0][TASK]) # early epoch\n",
    "    lateEpochWidth = numSamps[TASK] - epochBorders[1][TASK] + 1\n",
    "    epoch[TASK][1] = np.arange(lateEpochWidth) + epochBorders[1][TASK] - 1\n",
    "    # epoch[TASK][1] = np.linspace(epochBorders[1][TASK],numSamps[TASK],numSamps[TASK] - epochBorders[1][TASK] + 1) # late epoch\n",
    "\n",
    "    # preallocate\n",
    "    driftHack[TASK] = [[]] * numSubs\n",
    "\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        # initialize plot\n",
    "        plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "        # main title\n",
    "        plt.suptitle(taskNames[TASK] + ' sub ' + str(SUB + 1))\n",
    "\n",
    "        # get time series for current sub\n",
    "        data = boldData[TASK][NORM][SUB]\n",
    "\n",
    "        # compute hacky drift statistic\n",
    "        driftHack[TASK][SUB] = np.mean(data[tuple(epoch[TASK][0]),:],axis=0) - np.mean(data[tuple(epoch[TASK][1]),:],axis=0)\n",
    "\n",
    "        # optional standardization\n",
    "        if stdDiff:\n",
    "            driftHack[TASK][SUB] = (driftHack[TASK][SUB] - np.mean(driftHack[TASK][SUB])) / np.std(driftHack[TASK][SUB])\n",
    "\n",
    "        # select subplot for histogram\n",
    "        plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "        # plt.hold(True)\n",
    "\n",
    "        # plot difference histogram\n",
    "        plt.hist(driftHack[TASK][SUB], bins=100, density=True, alpha=0.5, color=taskColors[TASK])\n",
    "        plt.xlabel('mean(first ' + str(epochBorders[0][TASK]) + ' time points) - mean(last ' + str(numSamps[TASK] - epochBorders[1][TASK] + 1) + ' timpoints')\n",
    "        plt.ylabel('proportion of voxels')\n",
    "\n",
    "        # get voxel indices for time series with min and max difference scores and those at various percentile cutoffs\n",
    "        evox = [[]] * 5\n",
    "        evox[0] = np.unravel_index(np.argmin(driftHack[TASK][SUB]),driftHack[TASK][SUB].shape)[0]\n",
    "        if voxMethod == 'median':\n",
    "            evox[1] = (np.abs(driftHack[TASK][SUB] - (np.mean(driftHack[TASK][SUB]) - np.std(driftHack[TASK][SUB])))).argmin()\n",
    "            evox[2] = (np.abs(driftHack[TASK][SUB] - np.mean(driftHack[TASK][SUB]))).argmin()\n",
    "            evox[3] = (np.abs(driftHack[TASK][SUB] - (np.mean(driftHack[TASK][SUB]) + np.std(driftHack[TASK][SUB])))).argmin()\n",
    "        else:\n",
    "            counter = 1\n",
    "            for PERC in percentiles:\n",
    "                evox[counter] = find_nearest_percentile_index(driftHack[TASK][SUB], PERC)\n",
    "                counter += 1\n",
    "        evox[4] = np.unravel_index(np.argmax(driftHack[TASK][SUB]),driftHack[TASK][SUB].shape)[0]\n",
    "\n",
    "        # get histogram max y-value\n",
    "        yMax = plt.gca().get_ylim()[1]\n",
    "\n",
    "        # plot single voxel timeseries\n",
    "        for VOX in range(len(evox)):\n",
    "\n",
    "            # add vertical bars to histogram\n",
    "            plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "            diffVal = driftHack[TASK][SUB][evox[VOX]]\n",
    "            plt.plot([diffVal, diffVal], [0, yMax], '-', color=voxColors[VOX])\n",
    "\n",
    "            # get time series at the extreme voxel\n",
    "            y = boldData[TASK][NORM][SUB][:,evox[VOX]]\n",
    "            x = np.array(range(len(y))) + 1\n",
    "\n",
    "            # select subplot for time series line plot\n",
    "            ax = plt.subplot(spMap.shape[0], spMap.shape[1], VOX + 2)\n",
    "            plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "            plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "            # plot time series\n",
    "            plt.plot(x,y,'-k')\n",
    "\n",
    "            # subplot title and axis labels\n",
    "            plt.title(diffLabs[VOX])\n",
    "            if TASK == 0:\n",
    "                plt.xlabel('TR')\n",
    "            else:\n",
    "                plt.xlabel('reading stimulus flip')\n",
    "            plt.ylabel('BOLD signal')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% make a hacky drift measure\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "driftHackCons = [[]] * 2\n",
    "smoothnessCons = [[]] * 2\n",
    "driftHack_mean = [[]] * 2\n",
    "smoothness_mean = [[]] * 2\n",
    "\n",
    "# get mean drift measure across subs\n",
    "for TASK in [0,1]:\n",
    "\n",
    "    driftHackCons[TASK] = np.empty([numSubs,numVox])\n",
    "    smoothnessCons[TASK] = np.empty([numSubs,numVox])\n",
    "\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        # make sure everything is standardized\n",
    "        driftHackCons[TASK][SUB,:] = (driftHack[TASK][SUB] - np.mean(driftHack[TASK][SUB])) / np.std(driftHack[TASK][SUB])\n",
    "        smoothnessCons[TASK][SUB,:] = (smoothness[TASK][SUB] - np.mean(smoothness[TASK][SUB])) / np.std(smoothness[TASK][SUB])\n",
    "\n",
    "    driftHack_mean[TASK] = np.mean(driftHackCons[TASK], axis=0)\n",
    "    smoothness_mean[TASK]  = np.mean(smoothnessCons[TASK], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% get summary measures (means)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smoothness_mean[TASK][0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if saveStatMaps:\n",
    "    from nilearn import image as nImage\n",
    "    from nilearn import input_data\n",
    "\n",
    "    # get masker object\n",
    "    maskImg = nImage.load_img(maskFile)\n",
    "    masker = input_data.NiftiMasker(maskImg)\n",
    "    masker.fit_transform(maskImg)\n",
    "\n",
    "    for TASK in [0,1]:\n",
    "        smoothPath = statMapFolder + 'smoothMean_' + taskNames[TASK] + '.nii.gz'\n",
    "        driftPath = statMapFolder + 'driftMean_' + taskNames[TASK] + '.nii.gz'\n",
    "\n",
    "        # smoothness\n",
    "        cleaned_img = masker.inverse_transform(smoothness_mean[TASK])\n",
    "        cleaned_img.to_filename(smoothPath)\n",
    "\n",
    "        # \"drift\"\n",
    "        cleaned_img = masker.inverse_transform(driftHack_mean[TASK])\n",
    "        cleaned_img.to_filename(driftPath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save out smoothness and \"drift\" statistical maps\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set parameters for confirming a few things about voxel-level analyses\n",
    "SUB=0\n",
    "TASK=0\n",
    "NORM=1\n",
    "VOX=1 #0=min,1=max\n",
    "\n",
    "# get extreme correlation index\n",
    "pData = corrData[TASK][NORM][SUB]\n",
    "if VOX == 0:\n",
    "    EIND = np.unravel_index(np.argmin(pData),pData.shape) # minimum correlation voxel index\n",
    "else:\n",
    "    EIND = np.unravel_index(np.argmax(pData),pData.shape) # maximum correlation voxel index\n",
    "\n",
    "# get individual timeseries at extreme voxel\n",
    "y1 = boldData[TASK][NORM][SUB][:,EIND].flatten()\n",
    "\n",
    "# get rest-of-group mean timeseries at extreme voxel\n",
    "otherSubs = np.arange(0,numSubs)\n",
    "otherSubs = np.delete(otherSubs,SUB)\n",
    "y2 = np.mean([boldData[TASK][NORM][i][:,EIND] for i in otherSubs], axis=0).flatten()\n",
    "\n",
    "# normalize (z-score) the resulting rest-of-group mean timeseries\n",
    "y2_norm = scaler.fit_transform(y2.reshape(len(y2),1)).flatten()\n",
    "\n",
    "#confirm that using normalized (z-scored) group data doesn't change correlation values\n",
    "print(np.corrcoef(y1,y2))\n",
    "print(np.corrcoef(y1,y2_norm))\n",
    "\n",
    "#confirm that using the fastColumnCorr function doesn't change correlation values\n",
    "print(fastColumnCorr(y1.reshape(y1.shape[0],1),y2.reshape(y1.shape[0],1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% quick sanity check\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a random vector, a\n",
    "a = np.random.rand(5).reshape(5,1)\n",
    "print('random vector a:')\n",
    "print(np.transpose(a)[0])\n",
    "\n",
    "# make a random vector, b\n",
    "b = np.random.rand(5).reshape(5,1)\n",
    "print('\\nrandom vector b:')\n",
    "print(np.transpose(b)[0])\n",
    "\n",
    "# z-score normalize vectors a and b\n",
    "a_norm = scaler.fit_transform(a.reshape(len(a),1)).flatten()\n",
    "print('\\nz-scored vector a:')\n",
    "print(a_norm)\n",
    "b_norm = scaler.fit_transform(b.reshape(len(b),1)).flatten()\n",
    "print('\\nz-scored vector b:')\n",
    "print(b_norm)\n",
    "\n",
    "# get the mean of non-normalized vectors a and b\n",
    "ab_mean = np.mean([a, b],axis=0)\n",
    "print('\\nmean of raw vectors a and b:')\n",
    "print(np.transpose(ab_mean)[0])\n",
    "\n",
    "# normalize the mean of non-normalized vectors a and b\n",
    "ab_mean_norm = scaler.fit_transform(ab_mean.reshape(len(ab_mean),1)).flatten()\n",
    "print('\\nz-scored mean of raw vectors a and b:')\n",
    "print(ab_mean_norm)\n",
    "\n",
    "# get the mean of normalized vectors a and b\n",
    "ab_norm_mean = np.mean([a_norm, b_norm],axis=0)\n",
    "print('\\nmean of z-scored vectors a and b:')\n",
    "print(ab_norm_mean)\n",
    "\n",
    "# normalize the mean of normalized vectors a and b\n",
    "ab_norm_mean_norm = scaler.fit_transform(ab_norm_mean.reshape(len(ab_norm_mean),1)).flatten()\n",
    "print('\\nz-scored mean of z-scored vectors a and b:')\n",
    "print(ab_norm_mean_norm)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% do a little test on taking the mean of normalized time series vs normalizing a mean of raw timeseries\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if exportHtml:\n",
    "    os.chdir(htmlFolder)\n",
    "    os.system('jupyter nbconvert --to html control_ISC_correlation_mystery.ipynb')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}