{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "This script loads output data from control_task_ISC.py run\n",
    "on the Discovery cluster and runs various analyses depending\n",
    "on user input at the top of the script. See the \"set\n",
    "parameters\" chunk below for descriptions of the various\n",
    "analyses.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.multitest as multi\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "import pickle\n",
    "import sys\n",
    "from scipy.interpolate import interp1d\n",
    "sys.path.append('/dartfs-hpc/rc/home/z/f00589z/hyperscanning/support_scripts/')\n",
    "from phaseScramble import *\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation) # suppress some matplotlib warnings\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "baseFolder = '/dartfs-hpc/rc/home/z/f00589z/hyperscanning/control_tasks/'\n",
    "loadFolder = baseFolder + 'control_ISC_output/'\n",
    "inputFolder = baseFolder + 'nuisRegr_output_files/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% set paths\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# select parameters of dataset to load\n",
    "permutations = 5000 # number of permutations\n",
    "cShift = True # cShift (True) or pScram (False)\n",
    "normalize = True # time series z-scale normalized before ISC\n",
    "alpha = 0.05 # alpha for permutation tests\n",
    "twoTailed = True # two (True) or one-tailed (False; right tailed) permutation tests applied\n",
    "useDetrendedData = True # use data that was detrended during nilearn nuisance regression step\n",
    "fitNormal = False # load data that includes null distribution normal fit parameters\n",
    "removeListeningSamples = 0\n",
    "removeReadingSamples = 0\n",
    "\n",
    "# set analyses to include\n",
    "plotCorrDists = False # plot ISC distributions for each subject for each control task\n",
    "plotMinMaxMedianISCtimeSeries = False # look at sub vs group time series in voxels with min, max, and median ISC coefficients\n",
    "subBySubCorrMats = False # plot mean correlation values across voxels between subs\n",
    "plotPairwiseISCtimeSeries = False # NOTE that subBySubCorrMats must be True for this to run\n",
    "analyzeSmoothness = False # compute and plot time series smoothness measure (this was a concern early on but has largely been obviated as of June 2021)\n",
    "analyzeDrift = False # compute and plot time series drift measure\n",
    "findOptimalDriftWindow = False # find the shortest initial time windows to remove from each task to minimize drift ('analyzeDrift' must be True)\n",
    "ISC_statMap = True # plot median ISC heatmaps on an average brain surface\n",
    "drift_statMap = True # plot mean drift on an average brain surface\n",
    "smooth_statMap = False # plot mean smoothness on an average brain surface"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% et parameterss\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded file: /dartfs-hpc/rc/home/z/f00589z/hyperscanning/control_tasks/control_ISC_output/controlISC_5000perm_cShift_norm_twoTailed_detrended.pkl\n"
     ]
    }
   ],
   "source": [
    "# get file name based on input above\n",
    "fileName = 'controlISC_' + str(permutations) + 'perm'\n",
    "if cShift:\n",
    "    fileName = fileName + '_cShift'\n",
    "else:\n",
    "    fileName = fileName + '_pScram'\n",
    "if normalize:\n",
    "    fileName = fileName + '_norm'\n",
    "if twoTailed:\n",
    "    fileName = fileName + '_twoTailed'\n",
    "if useDetrendedData:\n",
    "    fileName = fileName + '_detrended'\n",
    "    epiTag = 'detrended_'\n",
    "else:\n",
    "    epiTag = ''\n",
    "if fitNormal:\n",
    "    fileName = fileName + '_nullDistFits'\n",
    "if removeListeningSamples > 0:\n",
    "    fileName = fileName + '_xL' + str(removeListeningSamples)\n",
    "if removeReadingSamples > 0:\n",
    "    fileName = fileName + '_xR' + str(removeReadingSamples)\n",
    "\n",
    "# load data\n",
    "with open(loadFolder + fileName + '.pkl', 'rb') as f:\n",
    "    permTest, corrData, groupFitData, duration, pGroup = pickle.load(f)\n",
    "    print('loaded file: ' + loadFolder + fileName + '.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# load hyperscanning subject list\n",
    "subList = pd.read_pickle('/dartfs-hpc/rc/home/z/f00589z/hyperscanning/misc/hyperscanning_subject_list.pkl')\n",
    "\n",
    "# get number of participants\n",
    "numSubs = subList.shape[0]\n",
    "\n",
    "# get number of pairs\n",
    "numPairs = round(numSubs / 2)\n",
    "\n",
    "# get number of voxels (using the first subject's ISC data from the listening task)\n",
    "numVox = len(corrData[0][0])\n",
    "\n",
    "# define condition labels\n",
    "taskNames = ['listening','reading']\n",
    "siteNames = ['DBIC','CBS']\n",
    "\n",
    "# indicate that we have not loaded the EPI time series\n",
    "epiLoaded = False\n",
    "\n",
    "# colorblind-friendly colors list\n",
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "# set task colors\n",
    "taskColors = CB_color_cycle[:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% setup\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open('/dartfs-hpc/rc/home/z/f00589z/hyperscanning/misc/hyperscanning_subject_list.pkl','wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([subList], f, protocol=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "subList.to_csv('/dartfs-hpc/rc/home/z/f00589z/hyperscanning/misc/hyperscanning_subject_list.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'/dartfs-hpc/rc/home/z/f00589z'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loadEPI(subList,folder,normalize,epiTag):\n",
    "\n",
    "    \"\"\"\n",
    "    function for loading EPI time series\n",
    "    :param subList: subs x 3 dataframe with columns: 'pairNum', 'site', and 'subID'\n",
    "    :param folder: folder from which to load EPI time series\n",
    "    :param normalize: boolean indicating whether or not to normalize time series\n",
    "    :return: boldData: time series as numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    numS = subList.shape[0]\n",
    "    taskNames = ['listening','reading']\n",
    "\n",
    "    # loop through participants...\n",
    "    boldData = [[]] * 2\n",
    "    for TASK in [0,1]: #for each task, listening, then reading\n",
    "\n",
    "        # preallocate task data list\n",
    "        boldData[TASK] = [[]] * numS\n",
    "\n",
    "        for S in range(numS):\n",
    "\n",
    "            # get file name\n",
    "            file = folder + 'sub-' + str(subList['subID'][S]) + '_ses-pair0' + str(subList['pairNum'][S]) + '_task-storytelling' + str(TASK + 3) + '_run-0' + str(TASK + 3) + '_bold_space-MNI152NLin2009cAsym_preproc_nuisRegr_2021_' + epiTag + 'interp.mat'\n",
    "\n",
    "            # load data\n",
    "            tmp = sio.loadmat(file) #load file\n",
    "            boldData[TASK][S] = tmp['tseries'] #get timeseries data\n",
    "            print('loaded ' + str(boldData[TASK][S].shape[0]) + ' x ' + str(boldData[TASK][S].shape[1]) + ' timeseries for ' + taskNames[TASK] + ' task, sub ' + subList['subID'][S])\n",
    "\n",
    "            if normalize:\n",
    "                boldData[TASK][S] = stats.zscore(boldData[TASK][S],axis=0)\n",
    "                print('z-scoring time series')\n",
    "\n",
    "    return boldData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% function for loading EPI time series\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is effectively here just in case you want\n",
    "clarity on how the multi.fdrcorrection function\n",
    "works. You verified that the function below\n",
    "gives the same output.\n",
    "\"\"\"\n",
    "\n",
    "def fdr(pVals,q):\n",
    "\n",
    "    # get p values\n",
    "    pVals = np.sort(pVals)\n",
    "\n",
    "    # get \"unsorting\" indices so you can map the hypothesis testing results back to the proper voxels\n",
    "    unsortInds = pVals.argsort().argsort()\n",
    "\n",
    "    # find threshold\n",
    "    N = len(pVals)\n",
    "    i = np.arange(1, N+1) # the 1-based i index of the p values, as in p(i)\n",
    "\n",
    "    # print number of uncorrected -values below q\n",
    "    print('# uncorrected pVals < ' + str(q) + ': ' + str(len(np.where(pVals < q)[0])))\n",
    "\n",
    "    # get pVals below qi / N\n",
    "    below = pVals < (q * i / N)\n",
    "\n",
    "    # if any critical value exists\n",
    "    if np.where(below)[0].size > 0:\n",
    "\n",
    "        # get index (sorted) of greatest pVal below qi / N\n",
    "        max_below = np.max(np.where(below)[0])\n",
    "\n",
    "        # get FDR adjusted p value\n",
    "        pCrit = pVals[max_below]\n",
    "\n",
    "        # print number of uncorrected -values below q\n",
    "        print('# uncorrected pVals < ' + str(pCrit) + ': ' + str(len(np.where(pVals <= pCrit)[0])))\n",
    "\n",
    "        # hypothesis test\n",
    "        h = (pVals <= pCrit)[unsortInds]\n",
    "\n",
    "    else:\n",
    "\n",
    "        h = np.zeros(N, dtype=bool)\n",
    "\n",
    "    return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% homebrewed fdr function\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def surfaceStatMap(masker,statMapVec,avgSurface,thresh):\n",
    "\n",
    "    # preallocate task arrays\n",
    "    statMap = [[]] * 2\n",
    "    texture = [[]] * 2\n",
    "    view = [[]] * 2\n",
    "\n",
    "    # for each task...\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        # get stat map\n",
    "        statMap[TASK] = masker.inverse_transform(statMapVec[TASK])\n",
    "\n",
    "        # surface plot\n",
    "        texture[TASK] = [[]] * 2\n",
    "        view[TASK] = [[]] * 2\n",
    "\n",
    "        for HEMI in [0,1]:\n",
    "            if HEMI == 0:\n",
    "                texture[TASK][HEMI] = surface.vol_to_surf(statMap[TASK], avgSurface.pial_left)\n",
    "                view[TASK][HEMI] = plotting.view_surf(avgSurface.infl_left,\n",
    "                                                                       texture[TASK][HEMI],\n",
    "                                                                       threshold=thresh,\n",
    "                                                                       colorbar=True,\n",
    "                                                                       title= taskNames[TASK] + ', left',\n",
    "                                                                       bg_map=avgSurface.sulc_left)\n",
    "            else:\n",
    "                texture[TASK][HEMI] = surface.vol_to_surf(statMap[TASK], avgSurface.pial_right)\n",
    "                view[TASK][HEMI] = plotting.view_surf(avgSurface.infl_right,\n",
    "                                                                       texture[TASK][HEMI],\n",
    "                                                                       threshold=thresh,\n",
    "                                                                       colorbar=True,\n",
    "                                                                       title=taskNames[TASK] + ', right',\n",
    "                                                                       bg_map=avgSurface.sulc_right)\n",
    "\n",
    "    return view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if plotCorrDists:\n",
    "\n",
    "    # set axis label font size\n",
    "    axLabFontSize = 12\n",
    "\n",
    "    # plot data\n",
    "    for SUB in range(numPairs): # for each pair...\n",
    "\n",
    "        # get subjects from current pair\n",
    "        pairSubs = [SUB,SUB + numPairs]\n",
    "\n",
    "        # initialize plot\n",
    "        plt.figure(facecolor='white',figsize=(6,6))\n",
    "\n",
    "        # for each subject in the current pair\n",
    "        for PAIRSUB in [0,1]:\n",
    "\n",
    "            for TASK in [0,1]:\n",
    "\n",
    "                # get plot data\n",
    "                pData = corrData[TASK][pairSubs[PAIRSUB]]\n",
    "\n",
    "                # select subplot\n",
    "                plt.subplot(2, 2, PAIRSUB*2 + TASK + 1)\n",
    "\n",
    "                # plot histogram\n",
    "                plt.hist(pData, bins=25, density=True, alpha=0.6, color=taskColors[TASK])\n",
    "\n",
    "                # dashed line at x=0\n",
    "                yMax = plt.gca().get_ylim()[1]\n",
    "                plt.plot([0, 0], [0, yMax], '--k')\n",
    "\n",
    "                # axes and title\n",
    "                plt.xlabel('correlation', fontsize=axLabFontSize)\n",
    "                if TASK == 0:\n",
    "                    plt.ylabel('voxel count', fontsize=axLabFontSize)\n",
    "                plt.title(taskNames[TASK] + ', sub ' + siteNames[PAIRSUB] + str(SUB + 1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% plot correlation distributions\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each participant and each task we plot the distribution\n",
    "of ISC coefficients (correlation between the participant's\n",
    "voxel time series and the mean voxel time series among all\n",
    "of the other participants) across voxels. We then overlay\n",
    "the individual voxel time series from the participant and\n",
    "the rest of the group that have the minimum, maximum, and\n",
    "median ISC coefficients across voxels.\n",
    "\"\"\"\n",
    "\n",
    "if plotMinMaxMedianISCtimeSeries:\n",
    "\n",
    "    # load EPI time series if necessary\n",
    "    if not epiLoaded:\n",
    "        boldData = loadEPI(subList,inputFolder,normalize,epiTag)\n",
    "        epiLoaded = True # indicate that we've loaded the EPI time series\n",
    "\n",
    "    # extreme voxel labels\n",
    "    voxLabs = ['min corr vox','max corr vox','median vox']\n",
    "    voxColors = ['y','m','k']\n",
    "\n",
    "    # set task colors\n",
    "    taskColors = CB_color_cycle[:2]\n",
    "\n",
    "    # make subplotting map\n",
    "    spMap3 = np.arange(8).reshape(4,2) + 1\n",
    "\n",
    "    # set axis label font size\n",
    "    axLabFontSize = 12\n",
    "\n",
    "    # define standard scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # plot data\n",
    "    for SUB in range(numSubs):\n",
    "\n",
    "        # initialize plot\n",
    "        plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "        # set main plot title\n",
    "        titleString = subList['subID'][SUB]\n",
    "        plt.suptitle(titleString)\n",
    "\n",
    "        for TASK in [0,1]:\n",
    "\n",
    "            # get plot data\n",
    "            pData = corrData[TASK][SUB]\n",
    "\n",
    "            # select subplot for histogram\n",
    "            plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[0,TASK])\n",
    "\n",
    "            # plot histogram\n",
    "            plt.hist(pData, bins=100, density=True, alpha=0.6, color=taskColors[TASK])\n",
    "\n",
    "            # dashed line at x=0\n",
    "            yMax = plt.gca().get_ylim()[1]\n",
    "            plt.plot([0, 0], [0, yMax], '--k')\n",
    "\n",
    "            # axes and title\n",
    "            plt.xlabel('correlation', fontsize=axLabFontSize)\n",
    "            if TASK == 0:\n",
    "                plt.ylabel('voxel count', fontsize=axLabFontSize)\n",
    "            plt.title(taskNames[TASK])\n",
    "\n",
    "            # plot voxel time series with extreme values\n",
    "            for VOX in [0,1,2]: # min, max, median\n",
    "\n",
    "                # get \"Extreme Index\" of voxel with either min or max value (or median)\n",
    "                if VOX == 0:\n",
    "                    EIND = np.unravel_index(np.argmin(pData),pData.shape) # minimum correlation voxel index\n",
    "                elif VOX == 1:\n",
    "                    EIND = np.unravel_index(np.argmax(pData),pData.shape) # maximum correlation voxel index\n",
    "                elif VOX == 2:\n",
    "                    EIND = np.argsort(pData)[len(pData)//2] # median (approximately)\n",
    "\n",
    "                # add locations of min and max correlation to histogram for reference\n",
    "                extremeCorr = pData[EIND]\n",
    "                plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[0,TASK])\n",
    "                plt.plot([extremeCorr, extremeCorr], [0, yMax], '-' + voxColors[VOX])\n",
    "\n",
    "                # get individual subject time series at the extreme voxel\n",
    "                y1 = boldData[TASK][SUB][:,EIND]\n",
    "                x = np.array(range(len(y1))) + 1\n",
    "\n",
    "                # get mean of data from all participants EXCEPT the current participant\n",
    "                otherSubs = np.arange(0,numSubs)\n",
    "                otherSubs = np.delete(otherSubs,SUB)\n",
    "                y2 = np.mean([boldData[TASK][i][:,EIND] for i in otherSubs], axis=0)\n",
    "                if VOX == 2: #hack to deal with EIND not being a tuple when we find the median\n",
    "                    y2 = y2.reshape(y2.shape[0],1)\n",
    "                y2 = scaler.fit_transform(y2) # normalize the rest-of-group mean (see next section for confirmation that this doesn't influence correlations)\n",
    "\n",
    "                # select subplot and reset subplot border color\n",
    "                ax = plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[VOX + 1,TASK])\n",
    "                plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "                plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "                # plot lines and add legend\n",
    "                line1, = plt.plot(x,y1,'-k',label = 'individual')\n",
    "                line2, = plt.plot(x,y2,'-', label = 'rest of group', color = taskColors[TASK]) # , linewidth=2\n",
    "                plt.legend(handles=[line1, line2],loc='upper right')\n",
    "\n",
    "                if TASK == 0:\n",
    "                    plt.xlabel('TR')\n",
    "                else:\n",
    "                    plt.xlabel('reading stimulus flip')\n",
    "                plt.ylabel('BOLD signal')\n",
    "                plt.title(voxLabs[VOX])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% look at sub vs group timeseries in voxels with min, max, and median ISC coefficients\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from matplotlib.colors import Normalize\n",
    "if subBySubCorrMats:\n",
    "\n",
    "    # load EPI time series if necessary\n",
    "    if not epiLoaded:\n",
    "        boldData = loadEPI(subList,inputFolder,normalize,epiTag)\n",
    "        epiLoaded = True # indicate that we've loaded the EPI time series\n",
    "\n",
    "    corrMat = [[]] * 2\n",
    "    corrColors = [[]] * 2\n",
    "    corrData_pairs = [[]] * 2\n",
    "    axLab = [[]] * numSubs\n",
    "\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        corrMat[TASK] = [[]] * 2\n",
    "        corrColors[TASK]= [[]] * 2\n",
    "        corrData_pairs[TASK]= [[]] * 2\n",
    "\n",
    "        # some feedback\n",
    "        print('\\ncomputing pairwise correlations for ' + str(taskNames[TASK]) + ' task')\n",
    "\n",
    "        # preallocate subs x subs correlation matrix\n",
    "        corrMat[TASK] = np.empty([numSubs,numSubs])\n",
    "        corrData_pairs[TASK] = [[]] * numSubs\n",
    "\n",
    "        for SUB1 in range(numSubs):\n",
    "\n",
    "            corrData_pairs[TASK][SUB1] = [[]] * numSubs\n",
    "\n",
    "            # get axis labels\n",
    "            if TASK == 0:\n",
    "                if SUB1 < numPairs:\n",
    "                    axLab[SUB1] = 'D' + str(SUB1 + 1)\n",
    "                else:\n",
    "                    axLab[SUB1] = 'H' + str(SUB1 - numPairs + 1)\n",
    "\n",
    "            # set the diagonal equal to 1\n",
    "            corrMat[TASK][SUB1,SUB1] = 1\n",
    "\n",
    "            for SUB2 in np.arange(SUB1 + 1,numSubs):\n",
    "\n",
    "                corrData_pairs[TASK][SUB1][SUB2] = fastColumnCorr(boldData[TASK][SUB1], boldData[TASK][SUB2])\n",
    "                corrMat[TASK][SUB1,SUB2] = np.mean(corrData_pairs[TASK][SUB1][SUB2])\n",
    "\n",
    "                #fill in the other half of corrMat so the plots dont look weird\n",
    "                corrMat[TASK][SUB2,SUB1] = corrMat[TASK][SUB1,SUB2]\n",
    "\n",
    "        plt.figure(facecolor='white')\n",
    "        cmap = cm.get_cmap('RdBu')#sns.diverging_palette(20, 220, n=200)\n",
    "        ax = sns.heatmap(\n",
    "            corrMat[TASK],\n",
    "            vmin=-1, vmax=1, center=0,\n",
    "            cmap=cmap,\n",
    "            square=True\n",
    "        )\n",
    "        ax.set_xticklabels(axLab)\n",
    "        ax.set_xticklabels(\n",
    "            ax.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            horizontalalignment='right'\n",
    "        )\n",
    "        ax.set_yticklabels(axLab)\n",
    "        ax.set_yticklabels(\n",
    "            ax.get_yticklabels(),\n",
    "            rotation=0\n",
    "        )\n",
    "\n",
    "        # add a title\n",
    "        plt.title('mean corr coef across vox, ' + taskNames[TASK] + ' task')\n",
    "\n",
    "        # get heatmap rgbs\n",
    "        im = ax.collections[0]\n",
    "        corrColors[TASK] = im.cmap(im.norm(im.get_array()))\n",
    "\n",
    "    # pairwise version of individual voxel time series comparisons\n",
    "    if plotPairwiseISCtimeSeries:\n",
    "\n",
    "        # make a numSubs by numSubs plot map\n",
    "        spMap4 = np.arange(numSubs**2).reshape(numSubs,numSubs)\n",
    "\n",
    "        # set plot width [inches?]\n",
    "        plotWidth = 16\n",
    "\n",
    "        # plot data\n",
    "        for SUB1 in range(numSubs):\n",
    "\n",
    "            # get sub1 string\n",
    "            if SUB1 < numPairs:\n",
    "                sub1Str = 'D' + str(SUB1 + 1)\n",
    "            else:\n",
    "                sub1Str = 'H' + str(SUB1 - numPairs + 1)\n",
    "\n",
    "            for SUB2 in np.arange(SUB1 + 1,numSubs):\n",
    "\n",
    "                # get sub2 string\n",
    "                if SUB2 < numPairs:\n",
    "                    sub2Str = 'D' + str(SUB2 + 1)\n",
    "                else:\n",
    "                    sub2Str = 'H' + str(SUB2 - numPairs + 1)\n",
    "\n",
    "                # initialize plot\n",
    "                plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "                # main title\n",
    "                plt.suptitle('subs ' + sub1Str + ' & ' + sub2Str)\n",
    "\n",
    "                for TASK in [0,1]:\n",
    "\n",
    "                    # get correlation data for a given pair\n",
    "                    pData = corrData_pairs[TASK][SUB1][SUB2]\n",
    "\n",
    "                    # plot histogram\n",
    "                    plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap[0,TASK])\n",
    "                    plt.hist(pData, bins=100, density=True, alpha=0.6, color=taskColors[TASK])\n",
    "\n",
    "                    # dashed line at x=0\n",
    "                    yMax = plt.gca().get_ylim()[1]\n",
    "                    plt.plot([0, 0], [0, yMax], '--k')\n",
    "\n",
    "                    # axes and title\n",
    "                    plt.xlabel('correlation', fontsize=axLabFontSize)\n",
    "                    if TASK == 0:\n",
    "                        plt.ylabel('voxel count', fontsize=axLabFontSize)\n",
    "                    plt.title(taskNames[TASK])\n",
    "\n",
    "                    for VOX in [0,1,2]: # min, max, median\n",
    "\n",
    "                        # get \"Extreme Index\" of voxel with either min or max value (or median)\n",
    "                        if VOX == 0:\n",
    "                            EIND = np.unravel_index(np.argmin(pData),pData.shape) # minimum correlation voxel index\n",
    "                        elif VOX == 1:\n",
    "                            EIND = np.unravel_index(np.argmax(pData),pData.shape) # maximum correlation voxel index\n",
    "                        elif VOX == 2:\n",
    "                            EIND = np.argsort(pData)[len(pData)//2] # median (approximately)\n",
    "\n",
    "                        # add locations of min and max correlation to histogram for reference\n",
    "                        extremeCorr = pData[EIND]\n",
    "                        plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[0,TASK])\n",
    "                        plt.plot([extremeCorr, extremeCorr], [0, yMax], '-', color=voxColors[VOX])\n",
    "\n",
    "                        # get individual subject time series at the extreme voxel\n",
    "                        y1 = boldData[TASK][SUB1][:,EIND]\n",
    "                        y2 = boldData[TASK][SUB2][:,EIND]\n",
    "                        x = np.array(range(len(y1))) + 1\n",
    "\n",
    "                        # select subplot for time series line plot\n",
    "                        ax = plt.subplot(spMap3.shape[0], spMap3.shape[1], spMap3[VOX + 1,TASK])\n",
    "                        plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "                        plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "                        line1, = plt.plot(x,y1,'-k',label = sub1Str)\n",
    "                        line2, = plt.plot(x,y2,'-', label = sub2Str, color = taskColors[TASK])\n",
    "                        plt.legend(handles=[line1, line2],loc='upper right')\n",
    "\n",
    "                        if TASK == 0:\n",
    "                            plt.xlabel('TR')\n",
    "                        else:\n",
    "                            plt.xlabel('reading stimulus flip')\n",
    "                        plt.ylabel('BOLD signal')\n",
    "                        plt.title(voxLabs[VOX])\n",
    "\n",
    "                # display plots\n",
    "                plt.tight_layout()\n",
    "                plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% sub x sub correlation matrices\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we use a basic formula for quantifying smoothness:\n",
    "sd(diff(x))/abs(mean(diff(x)))\n",
    "Adapted the formula from here:\n",
    "https://stats.stackexchange.com/questions/24607/how-to-measure-smoothness-of-a-time-series-in-r\n",
    "The inversion is so that large values reflect greater\n",
    "smoothness. Smoothness values can then be optionally\n",
    "standardized from 0 to 1 to make them a bit more\n",
    "interpratable.\n",
    "\n",
    "For each subject and each task we plot the distribution\n",
    "of smoothness values across voxels -- only showing those\n",
    "voxels less than or equal to the __th percentile, because\n",
    "these distributions are deeply right skewed. We then plot\n",
    "the individual time series for voxels at the min and max\n",
    "and three selected percentile values for smoothness.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if analyzeSmoothness:\n",
    "\n",
    "    # function to find nearest value to a given percentile in an array\n",
    "    def find_nearest_percentile_index(array, percentile):\n",
    "        array = np.asarray(array)\n",
    "        target = np.percentile(array, percentile)\n",
    "        idx = (np.abs(array - target)).argmin()\n",
    "        return idx\n",
    "\n",
    "    # set up a subplot map\n",
    "    spMap = np.arange(6).reshape(3,2) + 1\n",
    "\n",
    "    # preallocate task arrays for smoothness\n",
    "    smoothness = [[]] * 2\n",
    "\n",
    "    # option to standardize smoothness values\n",
    "    standardize0to1 = True\n",
    "\n",
    "    # select colors for different individual voxels\n",
    "    voxColors = CB_color_cycle[4:9]\n",
    "\n",
    "    # threshold percentile below which to include data for histogram (deals with extreme skew)\n",
    "    threshPerc = 90\n",
    "\n",
    "    # select smoothness percentiles to look at in case voxMethod == 'percentile' above\n",
    "    percentiles = [25, 50, threshPerc]\n",
    "\n",
    "    # make voxel labels\n",
    "    if standardize0to1:\n",
    "        smoothLabs = ['min smoothness = 0',str(percentiles[0]) + ' %', str(percentiles[1]) + ' %', str(percentiles[2]) + ' %','max smoothness = 1']\n",
    "    else:\n",
    "        smoothLabs = ['min smoothness',str(percentiles[0]) + ' %', str(percentiles[1]) + ' %', str(percentiles[2]) + ' %','max smoothness']\n",
    "\n",
    "    # get and plot smoothness values\n",
    "    for TASK in [0,1]: # for each task...\n",
    "\n",
    "        # preallocate sub arrays for smoothness\n",
    "        smoothness[TASK] = [[]] * numSubs\n",
    "\n",
    "        for SUB in range(numSubs): # for each subject...\n",
    "\n",
    "            # initialize plot\n",
    "            plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "            # main title (subID)\n",
    "            plt.suptitle(taskNames[TASK] + ' sub ' + str(SUB + 1))\n",
    "\n",
    "            # get data\n",
    "            data = boldData[TASK][SUB]\n",
    "\n",
    "            # compute smoothness\n",
    "            smoothness[TASK][SUB] = 1 / np.std(np.diff(data,axis=0),axis=0) / abs(np.mean(np.diff(data,axis=0),axis=0)) # see description above for source of formula\n",
    "\n",
    "            # optional z-score standardization\n",
    "            if standardize0to1:\n",
    "                smoothness[TASK][SUB] = (smoothness[TASK][SUB] - np.min(smoothness[TASK][SUB])) / (np.max(smoothness[TASK][SUB]) - np.min(smoothness[TASK][SUB]))\n",
    "\n",
    "            # arbitrarily subset for plotability (because these are so skewed)\n",
    "            data = smoothness[TASK][SUB]\n",
    "\n",
    "            # get voxel indices for time series with various levels of smoothness smoothness\n",
    "            evox = [[]] * 5\n",
    "            evox[0] = np.unravel_index(np.argmin(data),data.shape)[0]\n",
    "            counter = 1\n",
    "            for PERC in percentiles:\n",
    "                evox[counter] = find_nearest_percentile_index(data, PERC)\n",
    "                counter += 1\n",
    "            evox[4] = np.unravel_index(np.argmax(data),data.shape)[0]\n",
    "\n",
    "            # select subplot for histogram\n",
    "            plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "\n",
    "            # select subset of data to plot for histogram to deal with visualization problems from extreme skew\n",
    "            threshInd = find_nearest_percentile_index(data, threshPerc)\n",
    "            plotData = data[data <= data[threshInd]]\n",
    "\n",
    "            # plot smoothness histogram\n",
    "            plt.hist(plotData, bins=100, density=True, alpha=1, color=taskColors[TASK])\n",
    "            plt.xlabel('smoothness parameter')\n",
    "            plt.ylabel('density')\n",
    "            if standardize0to1:\n",
    "                plt.title('standardized (0 to 1) smoothness values up to ' + str(threshPerc) + ' percentile')\n",
    "            else:\n",
    "                plt.title('smoothness values up to ' + str(threshPerc) + ' percentile')\n",
    "\n",
    "            # get histogram max y-value\n",
    "            yMax = plt.gca().get_ylim()[1]\n",
    "\n",
    "            # plot single voxel timeseries\n",
    "            for VOX in range(len(evox)):\n",
    "\n",
    "                # add vertical bars to histogram\n",
    "                if data[evox[VOX]] <= data[threshInd]:\n",
    "                    plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "                    smoothVal = data[evox[VOX]]\n",
    "                    plt.plot([smoothVal, smoothVal], [0, yMax], '-', color=voxColors[VOX])\n",
    "\n",
    "                # get time series at the extreme voxel\n",
    "                y = boldData[TASK][SUB][:,evox[VOX]]\n",
    "                x = np.array(range(len(y))) + 1\n",
    "\n",
    "                # select subplot for time series line plot\n",
    "                ax = plt.subplot(spMap.shape[0], spMap.shape[1], VOX + 2)\n",
    "                plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "                plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "                # plot time series\n",
    "                plt.plot(x,y,'-k')\n",
    "\n",
    "                # subplot title and axis labels\n",
    "                plt.title(smoothLabs[VOX])\n",
    "                if TASK == 0:\n",
    "                    plt.xlabel('TR')\n",
    "                else:\n",
    "                    plt.xlabel('reading stimulus flip')\n",
    "                plt.ylabel('BOLD signal')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    ###############################################\n",
    "    ### get smoothness summary measures (means) ###\n",
    "    ###############################################\n",
    "\n",
    "    # preallocate\n",
    "    smoothnessCons = [[]] * 2\n",
    "    smoothness_mean = [[]] * 2\n",
    "\n",
    "    # get mean drift measure across subs\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        smoothnessCons[TASK] = np.empty([numSubs,numVox])\n",
    "\n",
    "        for SUB in range(numSubs):\n",
    "\n",
    "            # make sure everything is standardized\n",
    "            smoothnessCons[TASK][SUB,:] = (smoothness[TASK][SUB] - np.nanmean(smoothness[TASK][SUB])) / np.std(smoothness[TASK][SUB])\n",
    "\n",
    "        smoothness_mean[TASK]  = np.nanmean(smoothnessCons[TASK], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% look at the \"smoothness\" of each voxel timeseries for each subject\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "compare the mean signal of an early epoch to that of a late epoch. Greater absolute differences\n",
    "should indicate greater drift. NOTE that this is super hacky, but should be FAST and at least\n",
    "somewhat sensitive\n",
    "\n",
    "set ending and starting time points for the early and late epochs for each task, respectively\n",
    "epochBorders = [[10,5],[100,50]] would mean...\n",
    "early epochs for the listening and reading tasks would be time points 1-10 and 1-5, respectively\n",
    "and the late epochs would be 100-end, 50-end, also respectively\n",
    "\"\"\"\n",
    "\n",
    "if analyzeDrift:\n",
    "\n",
    "    # define epoch borders\n",
    "    epochBorders = [[10,5],[100,50]]\n",
    "\n",
    "    # subplot map\n",
    "    spMap = np.arange(6).reshape(3,2) + 1\n",
    "\n",
    "    # set percentiles, colors, labels\n",
    "    voxColors = CB_color_cycle[4:9]\n",
    "    percentiles = [10, 50, 90]\n",
    "    sdSf = 2 # standard deviation scaling factor\n",
    "    voxMethod = 'stdevs'\n",
    "    if voxMethod == 'stdevs':\n",
    "        diffLabs = ['most negative diff','mean - 1SD*' + str(sdSf), 'mean','mean + 1SD*' + str(sdSf) ,'most negative diff']\n",
    "    else:\n",
    "        diffLabs = ['most negative diff',str(percentiles[0]) + ' %', str(percentiles[1]) + ' %', str(percentiles[2]) + ' %','most positive diff']\n",
    "\n",
    "    # standardize difference scores\n",
    "    stdDiff = True\n",
    "\n",
    "    # preallocate arrays\n",
    "    epoch = [[]] * 2\n",
    "    driftHack = [[]] * 2\n",
    "\n",
    "    # load EPI time series if necessary\n",
    "    if not epiLoaded:\n",
    "        boldData = loadEPI(subList,inputFolder,normalize,epiTag)\n",
    "        epiLoaded = True # indicate that we've loaded the EPI time series\n",
    "\n",
    "    # get number of samples in the time series from each task, using the normalized data from the first subject\n",
    "    numSamps = [boldData[0][0].shape[0], boldData[1][0].shape[0]]\n",
    "\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        # get epoch time points\n",
    "        epoch[TASK] = [[]] * 2 # preallocate\n",
    "        epoch[TASK][0] = np.arange(epochBorders[0][TASK]) # early epoch\n",
    "        lateEpochWidth = numSamps[TASK] - epochBorders[1][TASK] + 1\n",
    "        epoch[TASK][1] = np.arange(lateEpochWidth) + epochBorders[1][TASK] - 1\n",
    "\n",
    "        # preallocate\n",
    "        driftHack[TASK] = [[]] * numSubs\n",
    "\n",
    "        for SUB in range(numSubs):\n",
    "\n",
    "            # initialize plot\n",
    "            plt.figure(facecolor='white',figsize=(16,8))\n",
    "\n",
    "            # main title\n",
    "            plt.suptitle(taskNames[TASK] + ' sub ' + str(SUB + 1))\n",
    "\n",
    "            # get time series for current sub\n",
    "            data = boldData[TASK][SUB]\n",
    "\n",
    "            # compute hacky drift statistic\n",
    "            driftHack[TASK][SUB] = np.mean(data[tuple(epoch[TASK][0]),:],axis=0) - np.mean(data[tuple(epoch[TASK][1]),:],axis=0)\n",
    "\n",
    "            # optional standardization\n",
    "            if stdDiff:\n",
    "                driftHack[TASK][SUB] = (driftHack[TASK][SUB] - np.mean(driftHack[TASK][SUB])) / np.std(driftHack[TASK][SUB])\n",
    "\n",
    "            # select subplot for histogram\n",
    "            plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "\n",
    "            # plot difference histogram\n",
    "            plt.hist(driftHack[TASK][SUB], bins=100, density=True, alpha=0.5, color=taskColors[TASK])\n",
    "            plt.xlabel('mean(first ' + str(epochBorders[0][TASK]) + ' time points) - mean(last ' + str(numSamps[TASK] - epochBorders[1][TASK] + 1) + ' timpoints')\n",
    "            plt.ylabel('proportion of voxels')\n",
    "\n",
    "            # get voxel indices for time series with min and max difference scores and those at various percentile cutoffs\n",
    "            evox = [[]] * 5\n",
    "            evox[0] = np.unravel_index(np.argmin(driftHack[TASK][SUB]),driftHack[TASK][SUB].shape)[0]\n",
    "            if voxMethod == 'stdevs':\n",
    "                evox[1] = (np.abs(driftHack[TASK][SUB] - (np.mean(driftHack[TASK][SUB]) - np.std(driftHack[TASK][SUB]) * sdSf))).argmin()\n",
    "                evox[2] = (np.abs(driftHack[TASK][SUB] - np.mean(driftHack[TASK][SUB]))).argmin()\n",
    "                evox[3] = (np.abs(driftHack[TASK][SUB] - (np.mean(driftHack[TASK][SUB]) + np.std(driftHack[TASK][SUB]) * sdSf))).argmin()\n",
    "            else:\n",
    "                counter = 1\n",
    "                for PERC in percentiles:\n",
    "                    evox[counter] = find_nearest_percentile_index(driftHack[TASK][SUB], PERC)\n",
    "                    counter += 1\n",
    "            evox[4] = np.unravel_index(np.argmax(driftHack[TASK][SUB]),driftHack[TASK][SUB].shape)[0]\n",
    "\n",
    "            # get histogram max y-value\n",
    "            yMax = plt.gca().get_ylim()[1]\n",
    "\n",
    "            # plot single voxel timeseries\n",
    "            for VOX in range(len(evox)):\n",
    "\n",
    "                # add vertical bars to histogram\n",
    "                plt.subplot(spMap.shape[0], spMap.shape[1], 1)\n",
    "                diffVal = driftHack[TASK][SUB][evox[VOX]]\n",
    "                plt.plot([diffVal, diffVal], [0, yMax], '-', color=voxColors[VOX])\n",
    "\n",
    "                # get time series at the extreme voxel\n",
    "                y = boldData[TASK][SUB][:,evox[VOX]]\n",
    "                x = np.array(range(len(y))) + 1\n",
    "\n",
    "                # select subplot for time series line plot\n",
    "                ax = plt.subplot(spMap.shape[0], spMap.shape[1], VOX + 2)\n",
    "                plt.setp(ax.spines.values(), color=voxColors[VOX])\n",
    "                plt.setp([ax.get_xticklines(), ax.get_yticklines()], color=voxColors[VOX])\n",
    "\n",
    "                # plot time series\n",
    "                plt.plot(x,y,'-k')\n",
    "\n",
    "                # subplot title and axis labels\n",
    "                plt.title(diffLabs[VOX])\n",
    "                if TASK == 0:\n",
    "                    plt.xlabel('TR')\n",
    "                else:\n",
    "                    plt.xlabel('reading stimulus flip')\n",
    "                plt.ylabel('BOLD signal')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    ##########################################\n",
    "    ### get drift summary measures (means) ###\n",
    "    ##########################################\n",
    "\n",
    "    # preallocate\n",
    "    driftHackCons = [[]] * 2\n",
    "    driftHack_mean = [[]] * 2\n",
    "\n",
    "    # get mean drift measure across subs\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        # preallocate\n",
    "        driftHackCons[TASK] = np.empty([numSubs,numVox])\n",
    "\n",
    "        for SUB in range(numSubs):\n",
    "\n",
    "            # make sure everything is standardized\n",
    "            driftHackCons[TASK][SUB,:] = (driftHack[TASK][SUB] - np.nanmean(driftHack[TASK][SUB])) / np.std(driftHack[TASK][SUB])\n",
    "\n",
    "        # get mean drift\n",
    "        driftHack_mean[TASK] = np.nanmean(driftHackCons[TASK], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% drift\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if findOptimalDriftWindow:\n",
    "\n",
    "    # load EPI time series if necessary\n",
    "    if not epiLoaded:\n",
    "        boldData = loadEPI(subList,inputFolder,normalize,epiTag)\n",
    "        epiLoaded = True # indicate that we've loaded the EPI time series\n",
    "\n",
    "    # set plotting scheme\n",
    "    individPlots = True\n",
    "    groupPlots = True\n",
    "\n",
    "    # set epoch widths and get the maximum number of TRs to remove\n",
    "    widths = [3,6] # epoch widths [TRs]\n",
    "    removalMax = 100\n",
    "\n",
    "    # get subplotting scheme\n",
    "    pRows = np.ceil(len(widths) / 2)\n",
    "    if len(widths) == 1:\n",
    "        pCols = 1\n",
    "    else:\n",
    "        pCols = 2\n",
    "\n",
    "    # standardize difference scores\n",
    "    stdDiff = True\n",
    "\n",
    "    # preallocate arrays\n",
    "    meanAbsDrift = [[]] * 2\n",
    "\n",
    "    # drift threshold scaling factor (to scale by 1 SD) - any voxels\n",
    "    # with drift values less than this distance from the mean drift\n",
    "    # value will be ignored in the analysis below. If set to zero,\n",
    "    # no thresholding will be applied.\n",
    "    threshSF = 2\n",
    "\n",
    "    for TASK in [0,1]: # for each task...\n",
    "\n",
    "        # preallocate\n",
    "        meanAbsDrift[TASK] = [[]] * numSubs\n",
    "\n",
    "        for SUB in range(numSubs): # for each subject...\n",
    "\n",
    "            # get time series for current sub\n",
    "            data = boldData[TASK][SUB]\n",
    "\n",
    "            # preallocate\n",
    "            meanAbsDrift[TASK][SUB] = np.empty([len(widths),removalMax])\n",
    "\n",
    "            # initialize individual plot and set title\n",
    "            if individPlots:\n",
    "                plt.figure(facecolor='white',figsize=(pCols * 4,pRows * 4))\n",
    "                if threshSF > 0:\n",
    "                    plt.suptitle(taskNames[TASK] + ' task, sub ' + str(SUB + 1) + ', drift threshold: +/-' + str(threshSF) + 'SD')\n",
    "                else:\n",
    "                    plt.suptitle(taskNames[TASK] + ' task, sub ' + str(SUB + 1) + ', no drift thresholding')\n",
    "\n",
    "            for WIDTH in range(len(widths)): # for each epoch width...\n",
    "\n",
    "                # feedback\n",
    "                print('\\nanalyzing ' + taskNames[TASK] + ' sub ' + str(SUB + 1) + ' width ' + str(WIDTH + 1))\n",
    "\n",
    "                for TRX in range(removalMax): # for each number of TRs removed\n",
    "\n",
    "                    # remove TRX TRs\n",
    "                    if TRX > 0:\n",
    "                        data = np.delete(data,0,0)\n",
    "\n",
    "                    # get epochs\n",
    "                    epochs = [np.arange(widths[WIDTH]), np.arange(widths[WIDTH],data.shape[0])]\n",
    "\n",
    "                    # compute drift statistic\n",
    "                    drift = np.mean(data[tuple(epochs[0]),:],axis=0) - np.mean(data[tuple(epochs[1]),:],axis=0)\n",
    "                    if TRX == 0: # if thresholding, select the voxels with the greatest drift prior to removing TRs\n",
    "                        if threshSF > 0:\n",
    "                            mu = np.mean(drift)\n",
    "                            sd = np.std(drift)\n",
    "                            thresholds = [mu - sd * threshSF, mu + sd * threshSF]\n",
    "                            voxInds = np.concatenate((np.argwhere(drift < thresholds[0]), np.argwhere(drift > thresholds[1])))\n",
    "                        else:\n",
    "                            voxInds = range(numVox)\n",
    "                    meanAbsDrift[TASK][SUB][WIDTH,TRX] = np.mean(np.abs(drift[voxInds]))\n",
    "\n",
    "                # optional standardization across TR removals for each width\n",
    "                if stdDiff:\n",
    "                    meanAbsDrift[TASK][SUB][WIDTH,:] = (meanAbsDrift[TASK][SUB][WIDTH,:] - np.mean(meanAbsDrift[TASK][SUB][WIDTH,:])) / np.std(meanAbsDrift[TASK][SUB][WIDTH,:])\n",
    "\n",
    "                # individual plots\n",
    "                if individPlots:\n",
    "                    plt.subplot(pRows,pCols,WIDTH+1)\n",
    "                    plt.plot(range(removalMax),meanAbsDrift[TASK][SUB][WIDTH,:],'-ok')\n",
    "                    plt.xlabel('# samples removed',fontsize=16)\n",
    "                    plt.ylabel('mean absolute drift',fontsize=16)\n",
    "                    plt.title('epoch width = ' + str(widths[WIDTH]) + ' samples',fontsize=16)\n",
    "\n",
    "            # clean setup for individual plots\n",
    "            if individPlots:\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "    # preallocate group stats arrays\n",
    "    groupMeanAbsDrift = [[]] * 2\n",
    "    groupSDAbsDrift = [[]] * 2\n",
    "\n",
    "    # get / plot group stats\n",
    "    driftSamps = [17,10] # hardcoding the thresholds for now\n",
    "    for TASK in [0,1]: # for each task...\n",
    "\n",
    "        # compute group mean drift\n",
    "        groupMeanAbsDrift[TASK] = np.mean([meanAbsDrift[TASK][i] for i in range(numSubs)], axis=0)\n",
    "        groupSDAbsDrift[TASK] = np.std([meanAbsDrift[TASK][i] for i in range(numSubs)], axis=0)\n",
    "\n",
    "        # plot group mean drift\n",
    "        if groupPlots:\n",
    "\n",
    "            # initialize plot\n",
    "            plt.figure(facecolor='white',figsize=(pCols * 4,pRows * 4))\n",
    "            if threshSF > 0:\n",
    "                plt.suptitle(taskNames[TASK] + ' task, group mean absolute drift (N=' + str(numSubs) + '), drift threshold: +/-' + str(threshSF) + 'SD')\n",
    "            else:\n",
    "                plt.suptitle(taskNames[TASK] + ' task, group mean absolute drift (N=' + str(numSubs) + '), no drift thresholding')\n",
    "\n",
    "            for WIDTH in range(len(widths)):\n",
    "                plt.subplot(pRows,pCols,WIDTH+1)\n",
    "                x = range(removalMax)\n",
    "                y = groupMeanAbsDrift[TASK][WIDTH,:]\n",
    "                error = groupSDAbsDrift[TASK][WIDTH,:]\n",
    "                plt.plot(x, y, 'k-')\n",
    "                plt.fill_between(x, y-error, y+error)\n",
    "                plt.xlabel('# samples removed',fontsize=16)\n",
    "                plt.ylabel('mean absolute drift',fontsize=16)\n",
    "                plt.title('epoch width = ' + str(widths[WIDTH]) + ' samples',fontsize=16)\n",
    "\n",
    "                # estimate the \"elbow\" of the group mean curve\n",
    "                # fit an exponential curve\n",
    "                popt, pcov = curve_fit(func, x, y, p0=(1, 1e-6, -1))\n",
    "                x2 = np.linspace(np.min(x),np.max(x),100)\n",
    "                y2 = func(x2, *popt)\n",
    "                plt.plot(x2,y2,'--r',linewidth=2)\n",
    "\n",
    "                yLims = plt.gca().get_ylim()\n",
    "                plt.plot([driftSamps[TASK],driftSamps[TASK]],yLims,'-r')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% find optimal drift window\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#############################################\n",
    "########### Note about stat maps! ###########\n",
    "#############################################\n",
    "\n",
    "Currently plotting stat maps by generating a 'view' list\n",
    "then printing its subcomponents in successive notebook\n",
    "chunks. Hence why the stat maps sections are broken into\n",
    "so many chunks.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Add note about hacky way you're plotting stat maps\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ISC_statMap or ISC_statMap or drift_statMap:\n",
    "\n",
    "    # import nilearn modules\n",
    "    from nilearn import image as nImage\n",
    "    from nilearn import input_data\n",
    "    from nilearn import datasets\n",
    "    from nilearn import surface\n",
    "    from nilearn import plotting\n",
    "\n",
    "    # get masker object\n",
    "    maskFile = '/dartfs-hpc/rc/home/z/f00589z/hyperscanning/control_tasks/nuisRegr_input_files/mni_asym09c_mask_resamp3x3.nii.gz'\n",
    "    maskImg = nImage.load_img(maskFile)\n",
    "    masker = input_data.NiftiMasker(maskImg)\n",
    "    masker.fit_transform(maskImg)\n",
    "\n",
    "    mapDir = '/dartfs-hpc/rc/home/z/f00589z/hyperscanning/control_tasks/statMaps/'\n",
    "    fsaverage = datasets.fetch_surf_fsaverage()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% get masker object\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if ISC_statMap:\n",
    "\n",
    "    #########################################\n",
    "    ### set hypothesis testing parameters ###\n",
    "    #########################################\n",
    "\n",
    "    # threshold by proportion of subjects with a significant FDR corrected p-value at a given voxel\n",
    "    propThresh = False\n",
    "\n",
    "    # if thresholding by the proportion of subjects with a significant FDR corrected p-value at a given voxel...\n",
    "    if propThresh:\n",
    "\n",
    "        # set proportion of participants who need to have fdr corrected\n",
    "        # significant median ISC coefficients at a voxel to include it\n",
    "        # in the mask\n",
    "        fdrProp = .5\n",
    "\n",
    "        # preallocate\n",
    "        fdrVecs = [[]] * 2\n",
    "        fdrMask = [[]] * 2\n",
    "\n",
    "        for TASK in [0,1]:\n",
    "\n",
    "            # preallocate\n",
    "            fdrVecs[TASK] = np.empty([numSubs,numVox])\n",
    "            fdrMask[TASK] = np.zeros([numVox,1])\n",
    "\n",
    "            for SUB in range(numSubs):\n",
    "\n",
    "                # compile fdr hypothesis testing vectors (1=reject null, 0=fail to reject null)\n",
    "                fdrVecs[TASK][SUB,:] = permTest[TASK][SUB][1][0]\n",
    "\n",
    "            # generate group mask based on fdrProp\n",
    "            for VOX in range(numVox):\n",
    "\n",
    "                if np.sum(fdrVecs[TASK][:,VOX]) > (numSubs * fdrProp):\n",
    "                    fdrMask[TASK][VOX] = 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        # evaluate real median ISC against null distribution of ISC group medians\n",
    "        homeBrew = False\n",
    "        alpha = 0.05\n",
    "\n",
    "    # make an array from 0 to numSubs\n",
    "    subNums = np.arange(0,numSubs)\n",
    "\n",
    "    # preallocate task arrays for mean ISC coefficients\n",
    "    corrData_median = [[]] * 2\n",
    "\n",
    "    # for each task...\n",
    "    for TASK in [0,1]:\n",
    "\n",
    "        # get mean ISC across subs\n",
    "        corrData_median[TASK] = np.median([corrData[TASK][i] for i in subNums], axis=0)\n",
    "\n",
    "        if propThresh:\n",
    "            corrData_median[TASK][fdrMask[TASK][:,0] == 0] = 0\n",
    "        else:\n",
    "\n",
    "            # get FDR hypothesis testing array\n",
    "            if homeBrew:\n",
    "                h = fdr(pGroup[TASK][METHOD][0], alphaPrime)\n",
    "            else:\n",
    "                # h = multi.fdrcorrection(pGroup[TASK][METHOD][0], alpha=alpha)[0]\n",
    "                h = pGroup[TASK][1][0]\n",
    "            print('\\n' + str(len(np.where(h == True)[0])) + ' hits')\n",
    "\n",
    "            # mask out voxels that failed to reject the null by setting them to 0\n",
    "            corrData_median[TASK][h == False] = 0 # try NaN?\n",
    "\n",
    "    # get surface plots\n",
    "    thresh = 0.001 # threshold the stat maps just above zero so that voxels where null is not rejected are not plotted\n",
    "    view = surfaceStatMap(masker,corrData_median,fsaverage,thresh)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% listening left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% listening right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reading left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "view[1][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reading right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if smooth_statMap:\n",
    "    if not analyzeSmoothness:\n",
    "        print('\\nYou need to run the \"analyzeSmoothness\" chunk before you can unlock this stat map.\\n')\n",
    "    else:\n",
    "        thresh = 0.99\n",
    "        view = surfaceStatMap(masker,smoothness_mean,fsaverage,thresh)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% plot smoothness stat map on fsaverage\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if smooth_statMap:\n",
    "    view[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% listening left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if smooth_statMap:\n",
    "    view[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% listening right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if smooth_statMap:\n",
    "    view[1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reading left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if smooth_statMap:\n",
    "    view[1][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reading right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if drift_statMap:\n",
    "    if not analyzeDrift:\n",
    "        print('\\nYou need to run the \"analyzeDrift\" chunk before you can unlock this stat map.\\n')\n",
    "    else:\n",
    "        thresh = 1\n",
    "        view = surfaceStatMap(masker,driftHack_mean,fsaverage,thresh)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% plot drift stat map on fsaverage\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if drift_statMap:\n",
    "    view[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% listening left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if drift_statMap:\n",
    "    view[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% listening right\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if drift_statMap:\n",
    "    view[1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reading left\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if drift_statMap:\n",
    "    view[1][1]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reading right\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hypescancentral",
   "language": "python",
   "display_name": "hypeScanKernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}